{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db03a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import papermill as pm\n",
    "import nbformat\n",
    "from nbconvert import HTMLExporter\n",
    "from minio import Minio\n",
    "from urllib.parse import urlparse\n",
    "from tempfile import TemporaryDirectory\n",
    "import os\n",
    "import shutil\n",
    "import psycopg2 \n",
    "import dotenv\n",
    "import json\n",
    "import io\n",
    "from datetime import date \n",
    "from nbconvert.preprocessors import Preprocessor\n",
    "import glob\n",
    "\n",
    "class RemoveEmptyCodeCellsPreprocessor(Preprocessor):\n",
    "    def preprocess(self, nb, resources):\n",
    "        nb.cells = [cell for cell in nb.cells if not(cell.cell_type=='code' and not cell.get('outputs'))]\n",
    "        return nb, resources\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "def run_notebook(gse_id, tmpdir):\n",
    "    root_dir = os.path.realpath(os.path.join(os.getcwd(), '..')) #check on this. still a bit uneasy.\n",
    "    print(root_dir)\n",
    "    print(f\"temp directory created at: {tmpdir}\")\n",
    "    input_path = os.path.join(root_dir, \"notebooks\", \"report_template.ipynb\") #where template notebook is located\n",
    "    temp_input_path = os.path.join(tmpdir, \"report_template.ipynb\") \n",
    "    shutil.copyfile(input_path, temp_input_path) #copy it into the temp directory\n",
    "    temp_output_path = os.path.join(tmpdir, f\"{gse_id}.ipynb\")\n",
    "    output_html = os.path.join(tmpdir, f\"{gse_id}.html\")\n",
    "\n",
    "    pm.execute_notebook(\n",
    "        input_path=temp_input_path,\n",
    "        output_path=temp_output_path,\n",
    "        parameters={\n",
    "            \"gse\": gse_id,\n",
    "            \"working_dir\": tmpdir\n",
    "        },\n",
    "    )\n",
    "    print(f\"Notebook executed and saved at {temp_output_path}\")\n",
    "\n",
    "    #save to html\n",
    "    with open(temp_output_path, 'r') as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "    preprocessor = RemoveEmptyCodeCellsPreprocessor()\n",
    "    nb, _ = preprocessor.preprocess(nb, {})\n",
    "    \n",
    "    html_exporter = HTMLExporter() #optional: template\n",
    "    html_exporter.exclude_input = True\n",
    "    html_exporter.exclude_output_prompt = True\n",
    "    html_exporter.exclude_input_prompt = True\n",
    "\n",
    "    html_data, _ = html_exporter.from_notebook_node(nb)\n",
    "    \n",
    "    with open(output_html, 'w') as f:\n",
    "        f.write(html_data)\n",
    "\n",
    "    print(f\"HTML generated and saved at {output_html}\")\n",
    "    os.remove(temp_input_path) #remove to avoid it being uploaded to S3\n",
    "\n",
    "    for file in glob.glob(os.path.join(tmpdir, \"*.soft.gz\")):\n",
    "        os.remove(file) #remove the soft.gz file that GEOParse downloads\n",
    "\n",
    "def update_postgres(tmpdir, conn, cur):\n",
    "    json_path = os.path.join(tmpdir, \"metadata.json\")\n",
    "    with open(json_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    cur.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public';\")\n",
    "    print(cur.fetchall()) #print the table names for debugging.\n",
    "\n",
    "    columns = metadata.keys()\n",
    "    values = [metadata[col] for col in columns]\n",
    "\n",
    "    query = f\"\"\"\n",
    "        INSERT INTO reports ({', '.join(columns)})\n",
    "        VALUES ({', '.join(['%s'] * len(columns))})\n",
    "        ON CONFLICT (id) DO UPDATE SET\n",
    "        {', '.join([f\"{col}=EXCLUDED.{col}\" for col in columns if col != 'id'])}\n",
    "    \"\"\"\n",
    "\n",
    "    cur.execute(query, values)\n",
    "    conn.commit()\n",
    "    print(\"successfully committed\")\n",
    "    os.remove(json_path)\n",
    "\n",
    "def update_s3(gse_id, tmpdir, s3, bucket):\n",
    "    \n",
    "    for root, _, files in os.walk(tmpdir):\n",
    "        for filename in files:\n",
    "            local_path = os.path.join(root, filename)\n",
    "            relative_path = os.path.relpath(local_path, tmpdir).replace(\"\\\\\", \"/\")\n",
    "            object_key = f\"{gse_id}/{relative_path}\"\n",
    "\n",
    "            s3.fput_object(bucket, object_key, local_path)\n",
    "    \n",
    "    print(f\"‚úÖ Uploaded GSE {gse_id} contents to MinIO bucket '{bucket}'\")\n",
    "\n",
    "        \n",
    "\n",
    "def process_gse(gse_id, conn, cur, s3, bucket):\n",
    "    cur.execute(\"SELECT 1 FROM reports WHERE id = %s LIMIT 1;\", (gse_id,))\n",
    "    exists = cur.fetchone() is not None\n",
    "    if exists:\n",
    "        print(f\"GSE {gse_id} already exists in Postgres. Skipping processing.\")\n",
    "        return\n",
    "    \n",
    "    with TemporaryDirectory() as tmpdir:\n",
    "        print(f\"started processing for {gse_id} in temp directory {tmpdir}\")\n",
    "        try:\n",
    "            run_notebook(gse_id, tmpdir)\n",
    "            update_s3(gse_id, tmpdir, s3, bucket)\n",
    "            update_postgres(tmpdir, conn, cur)\n",
    "            print(\"processing successful!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {gse_id}: {e}\")\n",
    "            raise #get rid of in production so it doesnt interrupt execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4d2355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìç Connected to: ('postgres', '172.18.0.3', 5432)\n",
      "üìÑ Tables in 'public': [('kysely_migration',), ('kysely_migration_lock',), ('reports',)]\n"
     ]
    }
   ],
   "source": [
    "dotenv.load_dotenv()\n",
    "\n",
    "password = os.getenv('POSTGRES_PASSWORD')\n",
    "conn=psycopg2.connect(os.environ['DATABASE_URL'])\n",
    "cur = conn.cursor()\n",
    "# See what DB you're connected to\n",
    "\n",
    "cur.execute(\"SELECT current_database(), inet_server_addr(), inet_server_port();\")\n",
    "print(\"üìç Connected to:\", cur.fetchone())\n",
    "\n",
    "# See which tables exist\n",
    "cur.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public';\")\n",
    "print(\"üìÑ Tables in 'public':\", cur.fetchall())\n",
    "\n",
    "\n",
    "# connect to db\n",
    "S3_URL_parsed = urlparse(os.environ['S3_URL'])\n",
    "s3 = Minio(\n",
    "  f\"{S3_URL_parsed.hostname}:{S3_URL_parsed.port}\",\n",
    "  access_key=f\"{S3_URL_parsed.username}\",\n",
    "  secret_key=f\"{S3_URL_parsed.password}\",\n",
    "  secure=S3_URL_parsed.scheme == 'https',\n",
    ")\n",
    "\n",
    "# create the bucket if it doesn't exist\n",
    "bucket, _, _ = S3_URL_parsed.path[1:].partition('/')\n",
    "if not s3.bucket_exists(bucket):\n",
    "  s3.make_bucket(bucket)\n",
    "  # enable anonymous downloading of files in this bucket\n",
    "  s3.set_bucket_policy(bucket, json.dumps({\n",
    "    'Version': '2012-10-17',\n",
    "    'Statement': [\n",
    "      {'Effect': 'Allow', 'Principal': {'AWS': '*'}, 'Action': 's3:GetBucketLocation', 'Resource': f\"arn:aws:s3:::{bucket}\"},\n",
    "      {'Effect': 'Allow', 'Principal': {'AWS': '*'}, 'Action': 's3:GetObject', 'Resource': f\"arn:aws:s3:::{bucket}/*\"},\n",
    "    ],\n",
    "  }))\n",
    "  # create a file\n",
    "  # content = b'Hello World!'\n",
    "  # s3.put_object(bucket, 'test.txt', io.BytesIO(content), len(content), content_type='plain/text')\n",
    "  # print(f\"File available at <{os.environ['PUBLIC_S3_URL']}/test.txt>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17487122",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.set_bucket_policy(bucket, json.dumps({\n",
    "    'Version': '2012-10-17',\n",
    "    'Statement': [\n",
    "      {'Effect': 'Allow', 'Principal': {'AWS': '*'}, 'Action': 's3:GetBucketLocation', 'Resource': f\"arn:aws:s3:::{bucket}\"},\n",
    "      {'Effect': 'Allow', 'Principal': {'AWS': '*'}, 'Action': 's3:GetObject', 'Resource': f\"arn:aws:s3:::{bucket}/*\"},\n",
    "    ],\n",
    "  }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce2b6e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "from urllib.parse import urlparse\n",
    "import json\n",
    "\n",
    "def reset_s3_bucket():\n",
    "    S3_URL_parsed = urlparse(os.environ['S3_URL'])\n",
    "    bucket, _, _ = S3_URL_parsed.path[1:].partition('/')\n",
    "    s3 = Minio(\n",
    "        f\"{S3_URL_parsed.hostname}:{S3_URL_parsed.port}\",\n",
    "        access_key=S3_URL_parsed.username,\n",
    "        secret_key=S3_URL_parsed.password,\n",
    "        secure=S3_URL_parsed.scheme == 'https',\n",
    "    )\n",
    "\n",
    "    if s3.bucket_exists(bucket):\n",
    "        objects = s3.list_objects(bucket, recursive=True)\n",
    "        for obj in objects:\n",
    "            s3.remove_object(bucket, obj.object_name)\n",
    "        print(f\"‚úÖ Cleared all objects from MinIO bucket '{bucket}'\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Bucket '{bucket}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "580ddbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_postgres():\n",
    "    conn = psycopg2.connect(os.environ['DATABASE_URL'])\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"TRUNCATE TABLE reports;\")  # deletes all rows, keeps schema\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    print(\"‚úÖ Postgres 'reports' table reset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86431e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "gse = \"GSE241523\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f75ef40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started processing for GSE241523 in temp directory /tmp/tmpczs2nv9k\n",
      "/home/ajy20/geo2reports/python\n",
      "temp directory created at: /tmp/tmpczs2nv9k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5bc92a96504061bcb31e2202591e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/97 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handler found for comm target 'dash'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook executed and saved at /tmp/tmpczs2nv9k/GSE241523.ipynb\n",
      "HTML generated and saved at /tmp/tmpczs2nv9k/GSE241523.html\n",
      "‚úÖ Uploaded GSE GSE241523 contents to MinIO bucket 'geo2reports'\n",
      "[('kysely_migration',), ('kysely_migration_lock',), ('reports',)]\n",
      "successfully committed\n",
      "processing successful!\n"
     ]
    }
   ],
   "source": [
    "#test everything here.\n",
    "process_gse(gse_id=gse, conn=conn, cur=cur, s3=s3, bucket=bucket)\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb477af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test if postgres was successfully updated\n",
    "conn = psycopg2.connect(os.environ['DATABASE_URL'])\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"SELECT * FROM reports WHERE id = %s\", (gse,))  # replace with actual GSE ID\n",
    "row = cur.fetchone()\n",
    "\n",
    "print(row)  # Should show the inserted metadata\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa68849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test if s3 was updated correctly.\n",
    "S3_URL_parsed = urlparse(os.environ['S3_URL'])\n",
    "bucket, _, _ = S3_URL_parsed.path[1:].partition('/')\n",
    "\n",
    "s3 = Minio(\n",
    "    f\"{S3_URL_parsed.hostname}:{S3_URL_parsed.port}\",\n",
    "    access_key=S3_URL_parsed.username,\n",
    "    secret_key=S3_URL_parsed.password,\n",
    "    secure=S3_URL_parsed.scheme == 'https',\n",
    ")\n",
    "\n",
    "# List all files in the folder for this GSE\n",
    "prefix = f\"{gse}/\"  # replace with your GSE ID\n",
    "objects = s3.list_objects(bucket, prefix=prefix, recursive=True)\n",
    "\n",
    "for obj in objects:\n",
    "    print(obj.object_name)  # Shows files like GSE123456/GSE123456.html, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80295013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Postgres 'reports' table reset.\n",
      "‚úÖ Cleared all objects from MinIO bucket 'geo2reports'\n"
     ]
    }
   ],
   "source": [
    "reset_postgres()\n",
    "reset_s3_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0de55",
   "metadata": {},
   "source": [
    "to do: \n",
    "- fix image resolutions and dpi. Make it consistent, maybe 600-700px wide (DONE)\n",
    "- delete the template to avoid it being uploaded to S3 (DONE)\n",
    "- stop saving htmls (DONE)\n",
    "- fix the clustergrammer color thresholds\n",
    "- fix citations from APA to AMA (DONE)\n",
    "- fix clustergrammer link (DONE)\n",
    "- get metadata matrix from elsewhere not from ARCHS4 H5 file. (DONE)\n",
    "- create a metadata list\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
